#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CyberSec 4B Model - Colab One-Click Training
=============================================
âœ… çœŸæ­£ä¸€éµé‹è¡Œ
âœ… è‡ªå‹•éŒ¯èª¤æ¢å¾©
âœ… ç„¡éœ€ Google Drive (å¯é¸)
âœ… è©³ç´°é€²åº¦åé¥‹

ä½¿ç”¨æ–¹æ³• (åœ¨ Colab ä¸­):
```python
!git clone https://github.com/Look0316/LLM_colab.git
%cd LLM_colab
!python colab_train.py
```
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Optional

# UTF-8 ç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8', errors='replace')
sys.stderr.reconfigure(encoding='utf-8', errors='replace')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONFIG = {
    "model_name": "Qwen/Qwen2.5-7B-Instruct",
    "num_samples": 2000,
    "epochs": 3,
    "batch_size": 4,
    "learning_rate": 2e-4,
    "output_dir": "/content/outputs",
    "data_file": "/content/data/distilled_tinyllm.jsonl",
    "use_drive": False,  # è¨­ç‚º True ä¾†å•Ÿç”¨ Google Drive
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å·¥å…·å‡½æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def print_step(step_num, message):
    """æ‰“å°æ­¥é©Ÿæ¨™é¡Œ"""
    print(f"\n{'='*60}")
    print(f"  Step {step_num}: {message}")
    print(f"{'='*60}")

def print_status(message, status="INFO"):
    """æ‰“å°ç‹€æ…‹"""
    emojis = {
        "INFO": "â„¹ï¸",
        "SUCCESS": "âœ…",
        "WARNING": "âš ï¸",
        "ERROR": "âŒ",
        "LOADING": "ğŸ”„",
    }
    print(f"{emojis.get(status, 'â„¹ï¸')} {message}")

def check_gpu():
    """æª¢æŸ¥ GPU ç‹€æ…‹"""
    import torch

    if not torch.cuda.is_available():
        print_status("æœªæª¢æ¸¬åˆ° GPU!", "ERROR")
        print("è«‹ç¢ºèª: Runtime â†’ Change runtime type â†’ GPU")
        return False, 4, 4

    gpu_name = torch.cuda.get_device_name(0)
    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)

    print_status(f"GPU: {gpu_name} ({gpu_mem:.1f} GB)", "SUCCESS")

    # æ ¹æ“š VRAM èª¿æ•´ batch size
    if gpu_mem >= 14:
        batch_size = 4
    elif gpu_mem >= 10:
        batch_size = 2
    else:
        batch_size = 1

    print_status(f"Batch Size: {batch_size}", "INFO")

    return True, batch_size, batch_size

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾è³´å®‰è£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def install_dependencies():
    """å®‰è£ä¾è³´ (åªå®‰è£å¿…è¦çš„)"""
    print_step(1, "å®‰è£ä¾è³´")

    import subprocess
    import sys

    packages = [
        "transformers>=4.40.0",
        "torch>=2.1.0",
        "accelerate>=0.28.0",
        "peft>=0.10.0",
        "bitsandbytes>=0.41.0",
        "trl>=0.8.0",
        "tqdm",
        "datasets",
    ]

    for pkg in packages:
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", "-q", pkg
            ])
            print_status(f"å·²å®‰è£: {pkg}", "SUCCESS")
        except Exception as e:
            print_status(f"å®‰è£å¤±æ•—: {pkg} - {e}", "WARNING")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•¸æ“šç”Ÿæˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_data(output_file, num_samples=2000):
    """ç”Ÿæˆ TinyLLM æ ¼å¼æ•¸æ“š"""
    print_step(2, "ç”Ÿæˆè¨“ç·´æ•¸æ“š")

    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import json
    import random
    from tqdm import tqdm

    print_status(f"æ¨¡å‹: {CONFIG['model_name']}", "INFO")
    print_status(f"æ¨£æœ¬æ•¸: {num_samples}", "INFO")

    # å‰µå»ºç›®éŒ„
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    # å ´æ™¯æ¨¡æ¿
    scenarios = [
        {"scenario": "SMBv1 enabled on Windows Server 2016", "category": "scan"},
        {"scenario": "Web app has SQL injection on login form", "category": "sqli"},
        {"scenario": "XSS found on comment form", "category": "xss"},
        {"scenario": "Redis server accessible without auth", "category": "redis"},
        {"scenario": "SSH weak password (root:toor)", "category": "ssh"},
        {"scenario": "JWT token with weak secret", "category": "jwt"},
        {"scenario": "Docker daemon exposed on port 2375", "category": "docker"},
        {"scenario": "Sudo version 1.8.31 vulnerable", "category": "privesc"},
        {"scenario": "MongoDB NoAuth on port 27017", "category": "mongo"},
        {"scenario": "phpMyAdmin exposed /admin", "category": "web"},
    ]

    # æ”»æ“Šæ­¥é©Ÿ
    step_templates = {
        "scan": [
            "nmap -p 445 --script smb-vuln-ms17-010 {target}",
            "enum4linux -a {target}",
        ],
        "sqli": [
            "sqlmap -u '{url}' --dbs",
            "sqlmap -u '{url}' -D {db} --tables",
        ],
        "xss": [
            "<script>alert(1)</script>",
            "<img src=x onerror=alert(1)>",
        ],
        "redis": [
            "redis-cli -h {target} INFO",
            "redis-cli -h {target} CONFIG GET *",
        ],
        "ssh": [
            "ssh root@{target}",
            "hydra -l root -P wordlist.txt ssh://{target}",
        ],
        "jwt": [
            "python3 jwt_tool.py -t {token} -s secret",
        ],
        "docker": [
            "curl http://{target}:2375/version",
            "docker -H {target} ps",
        ],
        "privesc": [
            "sudo -l",
            "searchsploit sudo 1.8.31",
        ],
        "mongo": [
            "mongo {target}:27017 --eval 'db.adminCommand({listDatabases:1})'",
        ],
        "web": [
            "curl {url}/admin/backups.sql",
            "curl {url}/phpinfo.php",
        ],
    }

    # è¼‰å…¥æ¨¡å‹
    print_status("è¼‰å…¥æ¨¡å‹ä¸­...", "LOADING")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            CONFIG["model_name"], 
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            CONFIG["model_name"],
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
        print_status("æ¨¡å‹è¼‰å…¥æˆåŠŸ", "SUCCESS")
    except Exception as e:
        print_status(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}", "ERROR")
        raise

    # ç”Ÿæˆæ•¸æ“š
    data = []
    print_status("é–‹å§‹ç”Ÿæˆæ•¸æ“š...", "LOADING")

    for i in tqdm(range(num_samples), desc="ç”Ÿæˆ"):
        scenario = random.choice(scenarios)
        cat = scenario["category"]

        messages = [
            {"role": "system", "content": "You are a professional penetration tester."},
            {"role": "user", "content": f"Scenario: {scenario['scenario']}\\nWhat is your next step?"},
        ]

        # ç”Ÿæˆ
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                )
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        except Exception as e:
            print_status(f"ç”Ÿæˆå¤±æ•—: {e}", "WARNING")
            response = f"Step: Analyze {scenario['scenario']}"

        sample = {
            "messages": messages + [{"role": "assistant", "content": response}],
            "category": cat,
            "scenario": scenario["scenario"],
            "steps": step_templates.get(cat, []),
        }
        data.append(sample)

        # æ¸…ç†è¨˜æ†¶é«”
        if i % 50 == 0:
            torch.cuda.empty_cache()

    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print_status(f"æ•¸æ“šå·²ä¿å­˜: {output_file}", "SUCCESS")
    print_status(f"ç¸½æ¨£æœ¬æ•¸: {len(data)}", "INFO")

    # å¸è¼‰æ¨¡å‹
    del model
    torch.cuda.empty_cache()

    return output_file

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QLoRA è¨“ç·´
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_qlora(data_file, output_dir, epochs=3):
    """QLoRA è¨“ç·´"""
    print_step(3, "QLoRA è¨“ç·´")

    from peft import LoraConfig, get_peft_model, TaskType
    from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
    from datasets import Dataset
    import json

    print_status(f"æ•¸æ“š: {data_file}", "INFO")
    print_status(f"è¼¸å‡º: {output_dir}", "INFO")
    print_status(f"Epochs: {epochs}", "INFO")

    # è®€å–æ•¸æ“š
    with open(data_file, 'r', encoding='utf-8') as f:
        raw_data = [json.loads(line) for line in f]

    print_status(f"è¼‰å…¥ {len(raw_data)} æ¨£æœ¬", "SUCCESS")

    # Tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        CONFIG["model_name"],
        trust_remote_code=True
    )

    # Dataset
    dataset_data = {
        "text": [
            tokenizer.apply_chat_template(
                item["messages"],
                tokenize=False,
            )
            for item in raw_data
        ]
    }
    dataset = Dataset.from_dict(dataset_data)

    # QLoRA é…ç½®
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    )

    # æ¨¡å‹
    from transformers import AutoModelForCausalLM
    print_status("è¼‰å…¥åŸºç¤æ¨¡å‹...", "LOADING")
    model = AutoModelForCausalLM.from_pretrained(
        CONFIG["model_name"],
        torch_dtype=torch.float16,
        device_map="auto",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # è¨“ç·´åƒæ•¸
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=CONFIG["batch_size"],
        gradient_accumulation_steps=4,
        learning_rate=CONFIG["learning_rate"],
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=2,
        fp16=True,
        report_to="none",
        dataloader_pin_memory=False,
        optim="paged_adamw_8bit",
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=DataCollatorForSeq2Seq(tokenizer),
    )

    # è¨“ç·´
    print_status("é–‹å§‹è¨“ç·´...", "LOADING")
    trainer.train()

    # ä¿å­˜
    os.makedirs(output_dir, exist_ok=True)
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    print_status(f"æ¨¡å‹å·²ä¿å­˜: {output_dir}", "SUCCESS")

    return output_dir

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¸¬è©¦å‡½æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_model(model_path):
    """æ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹"""
    print_step(4, "æ¸¬è©¦æ¨¡å‹")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch

    print_status("è¼‰å…¥æ¨¡å‹...", "LOADING")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # æ¸¬è©¦å•é¡Œ
    test_prompts = [
        {"role": "user", "content": "SMBv1 enabled on Windows Server 2016. What is your next step?"},
        {"role": "user", "content": "Found SQL injection on login form. Exploit it."},
    ]

    for prompt in test_prompts:
        print(f"\nğŸ‘¤ {prompt['content']}")
        messages = [
            {"role": "system", "content": "You are a professional penetration tester."},
            prompt,
        ]

        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("assistant")[-1].strip()
        print(f"ğŸ¤– {response[:300]}...")

    print_status("æ¸¬è©¦å®Œæˆ", "SUCCESS")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»å‡½æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "="*60)
    print("  ğŸ” CyberSec 4B Model - Colab ä¸€éµè¨“ç·´")
    print("="*60)

    try:
        # Step 1: æª¢æŸ¥ GPU
        print_step(0, "æª¢æŸ¥ GPU")
        has_gpu, _, _ = check_gpu()
        if not has_gpu:
            print_status("è­¦å‘Š: ç¹¼çºŒä½¿ç”¨ CPU è¨“ç·´æœƒéå¸¸æ…¢", "WARNING")

        # Step 2: å®‰è£ä¾è³´
        install_dependencies()

        # Step 3: ç”Ÿæˆæ•¸æ“š
        data_file = generate_data(
            CONFIG["data_file"],
            CONFIG["num_samples"]
        )

        # Step 4: è¨“ç·´
        output_dir = train_qlora(
            data_file,
            CONFIG["output_dir"],
            CONFIG["epochs"]
        )

        # Step 5: å®Œæˆ
        print("\n" + "="*60)
        print("  ğŸ‰ è¨“ç·´å®Œæˆ!")
        print("="*60)
        print(f"\nğŸ“ æ¨¡å‹ä½ç½®: {output_dir}")
        print(f"\nä¸‹ä¸€æ­¥:")
        print("1. ä¸‹è¼‰æ¨¡å‹æ–‡ä»¶")
        print("2. ä½¿ç”¨ transformers è¼‰å…¥æ¨ç†")
        print("3. æ·»åŠ  RAG æ¨¡å¡Šç²å–æœ€æ–° CVE")

    except KeyboardInterrupt:
        print_status("ç”¨æˆ¶ä¸­æ–·", "WARNING")
    except Exception as e:
        print_status(f"éŒ¯èª¤: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ æç¤º:")
        print("1. ç¢ºä¿é¸æ“‡äº† GPU (Runtime â†’ Change runtime type â†’ GPU)")
        print("2. é‡æ–°é‹è¡Œç´°èƒ")
        print("3. å¦‚æŒçºŒå¤±æ•—ï¼Œè«‹å›å ±éŒ¯èª¤")

if __name__ == "__main__":
    main()
